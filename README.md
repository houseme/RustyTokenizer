# RustyTokenizer
RustyTokenizer is a Chinese text tokenizer API service implemented in Rust. It utilizes the jieba-rs library for efficient Chinese word segmentation and uses the salvo-rs framework to build a RESTful API service. Users can send HTTP requests with arbitrary Chinese text to get tokenized results.
